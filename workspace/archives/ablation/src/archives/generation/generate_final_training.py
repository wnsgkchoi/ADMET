#!/usr/bin/env python3
"""
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
ê° ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ í•™ìŠµ
íŠœë‹ë˜ì§€ ì•Šì€ ë°ì´í„°ì…‹ì€ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
"""

import json

# ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (main.pyì˜ default ê°’)
DEFAULT_HYPERPARAMS = {
    'lr': 0.0001,
    'dropout_ratio': 0.5,
    'batch_size': 512,
    'num_experts': 3,
    'alpha': 0.1,
    'beta': 0.01,
    'min_temp': 1.0,
    'decay': 0.0,
    'num_layer': 5,
    'emb_dim': 300,
    'gate_dim': 50,
    'split_type': 'scaffold'
}

def generate_final_training_commands():
    """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ëª…ë ¹ì–´ ìƒì„±"""
    
    # ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ
    with open('configs/dataset_config.json', 'r') as f:
        dataset_config = json.load(f)
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ
    with open('configs/best_hyperparameters.json', 'r') as f:
        best_configs = json.load(f)
    
    commands = []
    
    print("="*80)
    print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ëª…ë ¹ì–´ ìƒì„±")
    print("="*80)
    print()
    
    # ëª¨ë“  ë°ì´í„°ì…‹ ì²˜ë¦¬ (íŠœë‹ëœ ê²ƒ + íŠœë‹ ì•ˆ ëœ ê²ƒ)
    all_datasets = dataset_config['datasets']
    
    for dataset_name in sorted(all_datasets.keys()):
        dataset_info = all_datasets[dataset_name]
        category = dataset_info['category']
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²°ì •: íŠœë‹ëœ ê²ƒì´ ìžˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if dataset_name in best_configs:
            config = best_configs[dataset_name]['best_config']
            config_source = "tuned"
        else:
            config = DEFAULT_HYPERPARAMS.copy()
            config_source = "default"
        
        # ëª…ë ¹ì–´ ìƒì„±
        cmd_parts = [
            "conda run -n ADMET python workspace/src/main.py",
            f"--category {category}",
            f"--dataset_name {dataset_name}",
            f"--experiment_id final",  # ìµœì¢… ëª¨ë¸ìž„ì„ í‘œì‹œ
        ]
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€
        cmd_parts.extend([
            f"--lr {config['lr']}",
            f"--dropout_ratio {config['dropout_ratio']}",
            f"--batch_size {config['batch_size']}",
            f"--num_experts {config['num_experts']}",
            f"--alpha {config['alpha']}",
            f"--beta {config['beta']}",
            f"--min_temp {config['min_temp']}",
            f"--decay {config['decay']}",
            f"--num_layer {config['num_layer']}",
            f"--emb_dim {config['emb_dim']}",
            f"--gate_dim {config['gate_dim']}",
            f"--split {config['split_type']}",
        ])
        
        # ìµœì¢… ëª¨ë¸ì€ ì¶©ë¶„í•œ epoch ìˆ˜ë¡œ í•™ìŠµ
        cmd_parts.append("--epochs 300")
        cmd_parts.append("--patience 50")  # Early stopping patience ì¦ê°€
        
        # Train + Valid í•©ì³ì„œ ìµœì¢… í•™ìŠµ
        cmd_parts.append("--use_combined_trainvalid")
        
        # Pre-trained GIN ì‚¬ìš© (transfer learning)
        cmd_parts.append("--gin_pretrained_file workspace/src/pre-trained/supervised_contextpred.pth")
        
        # ì²´í¬í¬ì¸íŠ¸ëŠ” best modelë§Œ ì €ìž¥ (--ckpt_all ì œê±°)
        # cmd_parts.append("--ckpt_all")  # ì´ ì¤„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        
        # ì—ëŸ¬ ë¡œê·¸ (ì‹¤íŒ¨ ì‹œë§Œ)
        cmd = " ".join(cmd_parts) + f" > workspace/logs/final_train_{dataset_name}.log 2>&1"
        
        commands.append(cmd)
        
        # ì¶œë ¥ (config_source í‘œì‹œ)
        config_marker = "ðŸŽ¯" if config_source == "tuned" else "ðŸ“‹"
        print(f"{config_marker} {dataset_name:<40} {category:<15} [{config_source}]")
        print(f"  â””â”€ lr={config['lr']}, dropout={config['dropout_ratio']}, batch={config['batch_size']}")
        print(f"  â””â”€ experts={config['num_experts']}, alpha={config['alpha']}, beta={config['beta']}")
        print()
    
    # ëª…ë ¹ì–´ íŒŒì¼ ì €ìž¥
    output_file = "workspace/commands_final_training.txt"
    
    # í†µê³„ ê³„ì‚°
    tuned_count = sum(1 for ds in all_datasets.keys() if ds in best_configs)
    default_count = len(all_datasets) - tuned_count
    
    with open(output_file, 'w') as f:
        f.write("# Final Model Training Commands\n")
        f.write(f"# Total: {len(commands)} datasets\n")
        f.write(f"# - Tuned hyperparameters: {tuned_count} datasets\n")
        f.write(f"# - Default hyperparameters: {default_count} datasets\n")
        f.write(f"# Estimated time: {len(commands)} Ã— 10min = {len(commands) * 10 / 60:.1f} hours\n")
        f.write("\n")
        
        for cmd in commands:
            f.write(cmd + "\n")
    
    print("="*80)
    print(f"âœ… ìµœì¢… í•™ìŠµ ëª…ë ¹ì–´ ì €ìž¥: {output_file}")
    print(f"âœ… ì´ {len(commands)}ê°œ ë°ì´í„°ì…‹")
    print(f"   - íŠœë‹ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {tuned_count}ê°œ")
    print(f"   - ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {default_count}ê°œ")
    print(f"âœ… ì˜ˆìƒ ì†Œìš” ì‹œê°„: {len(commands) * 10 / 60:.1f}ì‹œê°„")
    print("="*80)
    print()
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•™ìŠµ ì‹œìž‘:")
    print(f"  cd /home/choi0425/workspace/ADMET && \\")
    print(f"  nohup bash -c 'tail -n +6 {output_file} | conda run --no-capture-output -n ADMET python -m simple_gpu_scheduler.scheduler --gpus 0 1 2 3' > workspace/logs/scheduler_final_training.log 2>&1 &")
    
    return commands

if __name__ == "__main__":
    generate_final_training_commands()
