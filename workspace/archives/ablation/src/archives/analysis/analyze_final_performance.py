#!/usr/bin/env python3
"""
Enhanced Features ìµœì¢… ëª¨ë¸ë“¤ì˜ Test Set ì„±ëŠ¥ ë¶„ì„
33ê°œ ë°ì´í„°ì…‹ì˜ ìµœì¢… ì„±ëŠ¥ ìš”ì•½ ë° ë¹„êµ
"""

import json
import os
import torch
import pandas as pd
from pathlib import Path

def analyze_final_model_performance():
    """ìµœì¢… ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„"""
    
    results = {}
    output_base = "/home/choi0425/workspace/ADMET/workspace/output"
    
    # ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ
    with open('configs/dataset_config.json', 'r') as f:
        dataset_config = json.load(f)
    
    categories = ['Absorption', 'Distribution', 'Metabolism', 'Excretion', 'Toxicity']
    
    print("Enhanced Features ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 80)
    print(f"{'Dataset':<35} {'Category':<12} {'Task':<12} {'Metric':<8} {'Performance':<12} {'Size(MB)':<10}")
    print("-" * 80)
    
    # ì„±ëŠ¥ í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
    classification_metrics = []
    regression_metrics = []
    
    for category in categories:
        category_path = os.path.join(output_base, category)
        if not os.path.exists(category_path):
            continue
            
        for dataset_dir in os.listdir(category_path):
            dataset_path = os.path.join(category_path, dataset_dir)
            if not os.path.isdir(dataset_path):
                continue
                
            model_path = os.path.join(dataset_path, "best_model.pt")
            if not os.path.exists(model_path):
                continue
            
            try:
                # ëª¨ë¸ ë¡œë“œí•˜ì—¬ ì„±ëŠ¥ í™•ì¸
                checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
                test_metric = checkpoint['test_metric']
                args_dict = checkpoint['args']
                task_type = args_dict.get('task_type', 'classification')
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
                
                # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                dataset_info = dataset_config['datasets'].get(dataset_dir, {})
                samples = dataset_info.get('total_samples', 'Unknown')
                
                # ë©”íŠ¸ë¦­ íƒ€ì… ê²°ì •
                if task_type == 'classification':
                    metric_name = 'AUROC'
                    classification_metrics.append(test_metric)
                else:
                    metric_name = 'MAE'
                    regression_metrics.append(test_metric)
                
                results[dataset_dir] = {
                    'category': category,
                    'task_type': task_type,
                    'metric_name': metric_name,
                    'test_metric': float(test_metric),
                    'file_size_mb': file_size,
                    'total_samples': samples
                }
                
                # ì¶œë ¥
                performance_str = f"{test_metric:.4f}" if test_metric < 100 else f"{test_metric:.1f}"
                print(f"{dataset_dir:<35} {category:<12} {task_type:<12} {metric_name:<8} {performance_str:<12} {file_size:.1f}")
                
            except Exception as e:
                print(f"âŒ Error loading {dataset_dir}: {e}")
    
    print("-" * 80)
    
    # í†µê³„ ê³„ì‚°
    if classification_metrics:
        avg_auroc = sum(classification_metrics) / len(classification_metrics)
        min_auroc = min(classification_metrics)
        max_auroc = max(classification_metrics)
        
        print(f"\nğŸ“Š Classification Performance (AUROC):")
        print(f"  Average: {avg_auroc:.4f}")
        print(f"  Range: {min_auroc:.4f} - {max_auroc:.4f}")
        print(f"  Datasets: {len(classification_metrics)}")
    
    if regression_metrics:
        avg_mae = sum(regression_metrics) / len(regression_metrics)
        min_mae = min(regression_metrics)
        max_mae = max(regression_metrics)
        
        print(f"\nğŸ“Š Regression Performance (MAE):")
        print(f"  Average: {avg_mae:.4f}")
        print(f"  Range: {min_mae:.4f} - {max_mae:.4f}")
        print(f"  Datasets: {len(regression_metrics)}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    print(f"\nğŸ“‚ Category Breakdown:")
    category_stats = {}
    for dataset, result in results.items():
        cat = result['category']
        if cat not in category_stats:
            category_stats[cat] = {'count': 0, 'classification': 0, 'regression': 0}
        category_stats[cat]['count'] += 1
        if result['task_type'] == 'classification':
            category_stats[cat]['classification'] += 1
        else:
            category_stats[cat]['regression'] += 1
    
    for cat, stats in sorted(category_stats.items()):
        print(f"  {cat}: {stats['count']} datasets ({stats['classification']} classification, {stats['regression']} regression)")
    
    print(f"\nâœ… Total: {len(results)} datasets analyzed")
    
    # ìµœê³ /ìµœì € ì„±ëŠ¥ ë°ì´í„°ì…‹
    print(f"\nğŸ† Best Performing Datasets:")
    
    # Classification ìµœê³  ì„±ëŠ¥
    if classification_metrics:
        best_class = max(results.items(), key=lambda x: x[1]['test_metric'] if x[1]['task_type'] == 'classification' else 0)
        print(f"  Classification: {best_class[0]} (AUROC: {best_class[1]['test_metric']:.4f})")
    
    # Regression ìµœê³  ì„±ëŠ¥ (MAE ìµœì†Œ)
    if regression_metrics:
        best_reg = min(results.items(), key=lambda x: x[1]['test_metric'] if x[1]['task_type'] == 'regression' else float('inf'))
        print(f"  Regression: {best_reg[0]} (MAE: {best_reg[1]['test_metric']:.4f})")
    
    # íŒŒì¼ í¬ê¸° í†µê³„
    total_size = sum(result['file_size_mb'] for result in results.values())
    avg_size = total_size / len(results) if results else 0
    
    print(f"\nğŸ’¾ Storage Statistics:")
    print(f"  Total size: {total_size:.1f} MB ({total_size/1024:.2f} GB)")
    print(f"  Average size: {avg_size:.1f} MB per model")
    
    # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    output_file = "workspace/final_model_performance_analysis.json"
    with open(output_file, 'w') as f:
        json.dump({
            'summary': {
                'total_datasets': len(results),
                'classification_datasets': len(classification_metrics),
                'regression_datasets': len(regression_metrics),
                'avg_auroc': avg_auroc if classification_metrics else None,
                'avg_mae': avg_mae if regression_metrics else None,
                'total_size_mb': total_size,
                'avg_size_mb': avg_size
            },
            'detailed_results': results
        }, f, indent=2)
    
    print(f"\nğŸ“„ Detailed results saved: {output_file}")
    
    return results

def main():
    results = analyze_final_model_performance()
    return results

if __name__ == "__main__":
    main()