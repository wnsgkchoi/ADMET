#!/usr/bin/env python3
"""
Enhanced Features í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²°ê³¼ ë¶„ì„ ë° ìµœì  ì„¤ì • ì¶”ì¶œ
í˜„ì¬ê¹Œì§€ ì™„ë£Œëœ ì‹¤í—˜ë“¤ì—ì„œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ ìµœì¢… í•™ìŠµ ëª…ë ¹ ìƒì„±
"""

import json
import os
import pandas as pd
from pathlib import Path

def analyze_enhanced_features_results():
    """Enhanced features ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
    
    # ê²°ê³¼ ìˆ˜ì§‘
    results = {}
    hyperparam_base = "/home/choi0425/workspace/ADMET/workspace/output/hyperparam"
    
    if not os.path.exists(hyperparam_base):
        print("âŒ No hyperparameter results found!")
        return {}
    
    categories = ['Absorption', 'Distribution', 'Metabolism', 'Excretion', 'Toxicity']
    
    for category in categories:
        category_path = os.path.join(hyperparam_base, category)
        if not os.path.exists(category_path):
            continue
            
        for csv_file in os.listdir(category_path):
            if csv_file.endswith('_progress.csv'):
                dataset_name = csv_file.replace('_progress.csv', '')
                csv_path = os.path.join(category_path, csv_file)
                
                try:
                    df = pd.read_csv(csv_path)
                    
                    # enhanced_feat_ ì‹¤í—˜ë§Œ í•„í„°ë§
                    enhanced_df = df[df['experiment_id'].str.contains('enhanced_feat_', na=False)]
                    
                    if len(enhanced_df) == 0:
                        continue
                    
                    # ë©”íŠ¸ë¦­ ê²°ì • (classification vs regression)
                    if 'AUROC' in enhanced_df.columns or enhanced_df['test_metric'].max() <= 1.0:
                        # Classification: AUROC ìµœëŒ€í™”
                        best_idx = enhanced_df['test_metric'].idxmax()
                        metric_type = 'AUROC'
                    else:
                        # Regression: MAE/MSE ìµœì†Œí™”
                        best_idx = enhanced_df['test_metric'].idxmin()
                        metric_type = 'MAE'
                    
                    best_row = enhanced_df.loc[best_idx]
                    
                    results[dataset_name] = {
                        'category': category,
                        'best_config': {
                            'lr': best_row['lr'],
                            'dropout_ratio': best_row['dropout_ratio'],
                            'batch_size': int(best_row['batch_size']),
                            'num_experts': int(best_row['num_experts']),
                            'alpha': best_row['alpha'],
                            'beta': best_row['beta'],
                            'min_temp': best_row['min_temp'],
                            'decay': best_row['decay'],
                            'num_layer': int(best_row['num_layer']),
                            'emb_dim': int(best_row['emb_dim']),
                            'gate_dim': int(best_row['gate_dim']),
                            'split_type': best_row['split_type']
                        },
                        'best_test_metric': float(best_row['test_metric']),
                        'metric_type': metric_type,
                        'experiment_id': best_row['experiment_id'],
                        'total_experiments': len(enhanced_df)
                    }
                    
                    print(f"âœ… {dataset_name:<35} {category:<12} {metric_type}: {best_row['test_metric']:.4f} ({len(enhanced_df)} experiments)")
                    
                except Exception as e:
                    print(f"âŒ Error processing {dataset_name}: {e}")
    
    return results

def generate_enhanced_final_training_commands(results):
    """Enhanced features ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ ëª…ë ¹ ìƒì„±"""
    
    if not results:
        print("âŒ No results to generate commands!")
        return []
    
    # ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ
    with open('configs/dataset_config.json', 'r') as f:
        dataset_config = json.load(f)
    
    commands = []
    
    print("\n" + "="*80)
    print("Enhanced Features ìµœì¢… ëª¨ë¸ í•™ìŠµ ëª…ë ¹ì–´ ìƒì„±")
    print("="*80)
    print()
    
    for dataset_name in sorted(results.keys()):
        result = results[dataset_name]
        config = result['best_config']
        category = result['category']
        
        # ëª…ë ¹ì–´ ìƒì„±
        cmd_parts = [
            "conda run -n ADMET python workspace/src/main.py",
            f"--category {category}",
            f"--dataset_name {dataset_name}",
            f"--experiment_id enhanced_final",
        ]
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€
        cmd_parts.extend([
            f"--lr {config['lr']}",
            f"--dropout_ratio {config['dropout_ratio']}",
            f"--batch_size {config['batch_size']}",
            f"--num_experts {config['num_experts']}",
            f"--alpha {config['alpha']}",
            f"--beta {config['beta']}",
            f"--min_temp {config['min_temp']}",
            f"--decay {config['decay']}",
            f"--num_layer {config['num_layer']}",
            f"--emb_dim {config['emb_dim']}",
            f"--gate_dim {config['gate_dim']}",
            f"--split {config['split_type']}",
        ])
        
        # ìµœì¢… ëª¨ë¸ ì„¤ì •
        cmd_parts.extend([
            "--epochs 300",
            "--patience 50",
            "--use_combined_trainvalid",
            "--gin_pretrained_file workspace/src/pre-trained/supervised_contextpred.pth"
        ])
        
        cmd = " ".join(cmd_parts)
        commands.append(cmd)
        
        # ì¶œë ¥
        metric_info = f"{result['metric_type']}: {result['best_test_metric']:.4f}"
        print(f"ğŸ¯ {dataset_name:<35} {category:<12} {metric_info}")
        print(f"  â””â”€ lr={config['lr']}, dropout={config['dropout_ratio']}, experts={config['num_experts']}")
    
    # ëª…ë ¹ì–´ íŒŒì¼ ì €ì¥
    output_file = "workspace/commands_enhanced_final_training.txt"
    
    with open(output_file, 'w') as f:
        f.write("# Enhanced Features Final Model Training Commands\n")
        f.write(f"# Total: {len(commands)} datasets\n")
        f.write(f"# Features: 7 atom + 4 edge\n")
        f.write(f"# Pre-trained GIN: enabled\n")
        f.write(f"# Estimated time: {len(commands)} Ã— 10min = {len(commands) * 10 / 60:.1f} hours\n")
        f.write("\n")
        
        for cmd in commands:
            f.write(cmd + "\n")
    
    print("\n" + "="*80)
    print(f"âœ… ìµœì¢… í•™ìŠµ ëª…ë ¹ì–´ ì €ì¥: {output_file}")
    print(f"âœ… ì´ {len(commands)}ê°œ ë°ì´í„°ì…‹")
    print(f"âœ… ì˜ˆìƒ ì†Œìš” ì‹œê°„: {len(commands) * 10 / 60:.1f}ì‹œê°„")
    print("="*80)
    print()
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìµœì¢… í•™ìŠµ ì‹œì‘:")
    print(f"  cd /home/choi0425/workspace/ADMET && \\")
    print(f"  nohup bash -c 'tail -n +6 {output_file} | conda run --no-capture-output -n ADMET python -m simple_gpu_scheduler.scheduler --gpus 0 1 2 3' > workspace/logs/scheduler_enhanced_final.log 2>&1 &")
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
    config_file = "configs/best_hyperparameters_enhanced.json"
    with open(config_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥: {config_file}")
    
    return commands

def main():
    print("Enhanced Features í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²°ê³¼ ë¶„ì„ ì‹œì‘...")
    print()
    
    # ê²°ê³¼ ë¶„ì„
    results = analyze_enhanced_features_results()
    
    if not results:
        print("\nâŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print(f"\nğŸ“Š ì´ {len(results)}ê°œ ë°ì´í„°ì…‹ì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    categories = {}
    for dataset, result in results.items():
        cat = result['category']
        categories[cat] = categories.get(cat, 0) + 1
    
    print("\nì¹´í…Œê³ ë¦¬ë³„ ì™„ë£Œ í˜„í™©:")
    for cat, count in sorted(categories.items()):
        print(f"  {cat}: {count}ê°œ ë°ì´í„°ì…‹")
    
    # ìµœì¢… í•™ìŠµ ëª…ë ¹ ìƒì„±
    commands = generate_enhanced_final_training_commands(results)
    
    return results, commands

if __name__ == "__main__":
    main()